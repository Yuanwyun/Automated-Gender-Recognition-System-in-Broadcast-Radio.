# -*- coding: utf-8 -*-
"""NN.ipynb

Automatically generated by Colaboratory.



# Commented out IPython magic to ensure Python compatibility.
!pip3 install numpy
!pip3 install pandas 
!pip3 install tqdm
!pip3 install sklearn
!pip3 install tensorflow
!pip3 install librosa
!pip3 install pydub
!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg
!pip install pyaudio

!git clone https://github.com/x4nth055/gender-recognition-by-voice




import keras
from keras import datasets
from keras.layers import Dense, Flatten, Dropout, Activation, BatchNormalization, LSTM
from keras.layers import PReLU, LeakyReLU, Conv2D, MaxPool2D, Lambda
from keras.regularizers import l2
from keras.utils.np_utils import to_categorical

from keras.models import model_from_json

from IPython.display import clear_output

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from matplotlib.ticker import MaxNLocator

import pickle
import sklearn as skl

from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_score, train_test_split

import pandas as pd
import os
import tqdm
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping
from tqdm import tqdm
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from pydub import AudioSegment
from pydub.utils import make_chunks

# For extract_features func:
import glob
import shutil
import librosa

url = 'https://raw.githubusercontent.com/x4nth055/gender-recognition-by-voice/master/balanced-all.csv'
df = pd.read_csv(url)
df.head()

df.tail()

# get total samples
n_samples = len(df)
# get total male samples
n_male_samples = len(df[df['gender'] == 'male'])
# get total female samples
n_female_samples = len(df[df['gender'] == 'female'])
print("Total samples:", n_samples)
print("Total male samples:", n_male_samples)
print("Total female samples:", n_female_samples)

!git clone https://github.com/x4nth055/gender-recognition-by-voice

!ls

label2int = {
    "male": 1,
    "female": 0
}


def load_data(vector_length=128):
    """A function to load gender recognition dataset from `data` folder
    # After the second run, this will load from results/features.npy and results/labels.npy files
    # as it is much faster!"""
    # make sure results folder exists
    if not os.path.isdir("results"):
        os.mkdir("results")
    # if features & labels already loaded individually and bundled, load them from there instead
    if os.path.isfile("results/features.npy") and os.path.isfile("results/labels.npy"):
        X = np.load("results/features.npy")
        y = np.load("results/labels.npy")
        return X, y

    
    # read dataframe
    df = pd.read_csv("/content/gender-recognition-by-voice/balanced-all.csv")
    # get total samples
    n_samples = len(df)
    # get total male samples
    n_male_samples = len(df[df['gender'] == 'male'])
    # get total female samples
    n_female_samples = len(df[df['gender'] == 'female'])
    print("Total samples:", n_samples)
    print("Total male samples:", n_male_samples)
    print("Total female samples:", n_female_samples)
    # initialize an empty array for all audio features
    X = np.zeros((n_samples, vector_length))
    # initialize an empty array for all audio labels (1 for male and 0 for female)
    y = np.zeros((n_samples, 1))
    for i, (filename, gender) in tqdm(enumerate(zip(df['filename'], df['gender'])), "Loading data", total=n_samples):
        features = np.load("/content/gender-recognition-by-voice/"+filename)
        X[i] = features
        y[i] = label2int[gender]
    # save the audio features and labels into files
    # so we won't load each one of them next run
    np.save("results/features", X)
    np.save("results/labels", y)
    return X, y

def split_data(X, y, test_size=0.1, valid_size=0.1):
    # split training set and testing set
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)
    # split training set and validation set
    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)
    # return a dictionary of values
    return {
        "X_train": X_train,
        "X_valid": X_valid,
        "X_test": X_test,
        "y_train": y_train,
        "y_valid": y_valid,
        "y_test": y_test
    }

from tqdm import tnrange


# load the dataset
X,y = load_data()
# split the data into training, validation and testing sets
data = split_data(X, y, test_size=0.1, valid_size=0.1)

def create_model(vector_length=128):
    """5 hidden dense layers from 256 units to 64, not the best model."""
    model = Sequential()
    model.add(Dense(256, input_shape=(vector_length,)))
    model.add(Dropout(0.3))
    model.add(Dense(256, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(128, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(128, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation="relu"))
    model.add(Dropout(0.3))
    # model.add(Dense(64, activation="relu"))
    # model.add(Dropout(0.3))
    # one output neuron with sigmoid activation function, 0 means female, 1 means male
    model.add(Dense(1, activation="sigmoid"))
    # using binary crossentropy as it's male/female classification (binary)
    model.compile(loss="binary_crossentropy", metrics=["accuracy"], optimizer="adam")
    # print summary of the model
    model.summary()
    return model

# construct the model
model = create_model()

# Define some useful functions
class PlotLossAccuracy(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.acc = []
        self.losses = []
        self.val_losses = []
        self.val_acc = []
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(int(self.i))
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('accuracy'))
        self.val_acc.append(logs.get('val_accuracy'))
        
        self.i += 1
        
        clear_output(wait=True)
        plt.figure(figsize=(16, 6))
        plt.plot([1, 2])
        plt.subplot(121) 
        plt.plot(self.x, self.losses, label="train loss")
        plt.plot(self.x, self.val_losses, label="validation loss")
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.title('Model Loss')
        plt.legend()
        plt.subplot(122)
        plt.plot(self.x, self.acc, label="training accuracy")
        plt.plot(self.x, self.val_acc, label="validation accuracy")
        plt.legend()
        plt.ylabel('accuracy')
        plt.xlabel('epoch')
        plt.title('Model Accuracy')
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        plt.show();

tensorboard = PlotLossAccuracy()

# use tensorboard to view metrics
#tensorboard = TensorBoard(log_dir="logs")
# define early stopping to stop training after 10 epochs of not improving
early_stopping = EarlyStopping(mode="min", patience=10, restore_best_weights=True)

batch_size = 64
epochs = 100
# train the model using the training set and validating using validation set
model.fit(data["X_train"], 
          data["y_train"], 
          epochs=epochs, 
          batch_size=batch_size, 
          validation_data=(data["X_valid"], 
                           data["y_valid"]),
          callbacks=[tensorboard, early_stopping])

# save the model to a file
model.save("results/model.h5")

# evaluating the model using the testing set
print(f"Evaluating the model using {len(data['X_test'])} samples...")
loss, accuracy = model.evaluate(data["X_test"], data["y_test"], verbose=0)
print(f"Loss: {loss:.4f}")
print(f"Accuracy: {accuracy*100:.2f}%")

# Prediction is (0.9, 0.8, 0.98...)
#for Z in predicted, Z = 0.9,   0.8,   0.98....

#r = r+1

predicted = model.predict(data["X_test"])

r = 0
for Z in predicted:
  if predicted[r] >= 0.5:
    predicted[r] = 1
  else:
    predicted[r] = 0   
  r=r+1

print(predicted[:,0])

actual = data["y_test"]

cm = metrics.confusion_matrix(actual, predicted)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

import librosa
import numpy as np

def extract_feature(file_name, **kwargs):
    """
    Extract feature from audio file `file_name`
        Features supported:
            - MFCC (mfcc)
            - Chroma (chroma)
            - MEL Spectrogram Frequency (mel)
            - Contrast (contrast)
            - Tonnetz (tonnetz)
        e.g:
        `features = extract_feature(path, mel=True, mfcc=True)`
    """
    mfcc = kwargs.get("mfcc")
    chroma = kwargs.get("chroma")
    mel = kwargs.get("mel")
    contrast = kwargs.get("contrast")
    tonnetz = kwargs.get("tonnetz")
    X, sample_rate = librosa.core.load(file_name)
    if chroma or contrast:
        stft = np.abs(librosa.stft(X))
    result = np.array([])
    if mfcc:
        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
        result = np.hstack((result, mfccs))
    if chroma:
        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
        result = np.hstack((result, chroma))
    if mel:
        # OLD: mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)
        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)
        result = np.hstack((result, mel))
    if contrast:
        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)
        result = np.hstack((result, contrast))
    if tonnetz:
        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)
        result = np.hstack((result, tonnetz))
    return result

# construct the model
model = create_model()

model.load_weights("results/model.h5")

features = extract_feature('/speaker_diarization/test.wav', mel=True).reshape(1, -1)
# predict the gender!
male_prob = model.predict(features)[0][0]
female_prob = 1 - male_prob
gender = "male" if male_prob > female_prob else "female"
# show the result!
print("Result:", gender)
print(f"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%")

from google.colab import drive
drive.mount('/content/drive')

# This cell removes the extension of all the files in a directory, and puts the gender and duration of each file as a dictionary into an array.
from os import listdir
from os.path import isfile, join, splitext

# Enter the path to your directory that contains all the audio files that we will use to test.
# This just gets all the files in the specified directory.
mypath = '/speaker_diarization/common_voice_durs'
onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]
print('All files in the directory:\n', onlyfiles, '\n')

# Create an empty array.
# For each file in the 'onlyfiles' array, use the 'splitext' function to get rid of the extension in the string.
# Then put each of these into the newly created empty array (into arr_without_extension).
arr_without_extension = []
for counter, i in enumerate(onlyfiles):
  x = splitext(onlyfiles[counter])
  arr_without_extension.append(x[0])

print('All files in an array without the .mp3 extension (can remove any extension):\n', arr_without_extension, '\n')

# Create an empty array.
# For each element in arr_with_extension, get the name of the element, split the name based on the "-" character.
# Put the split string into an array of dictionaries, as gender and duration.
gender_duration_dictionary = []
for counter, i in enumerate(arr_without_extension):
  txt = (arr_without_extension[counter])
  x = txt.split("_")
  gender_duration = {'gender': x[0], 'file_no': x[1], 'duration': x[2]} 
  gender_duration_dictionary.append(gender_duration)
  
print('An array with a dictionary of the gender, file no. and the duration of each file:\n', gender_duration_dictionary, '\n')

# This cell is used to take random files from the sample folder.
# It takes 5 male, and 5 female.
# It records the information of each file: gender, file number and duration
# This information is put into a 3D array and the original name of the chosen files are recorded, so they can be joined together later.
import random

file_record = []
sound_files = []

for i in range(10):
  # 0 < x < 29, since there is only a maximum of 30 files for each gender in the folder
  x = random.randint(0, 29)

  # If i is even add 30 to x, so we are indexing the male files. This means the gender picked for each file is changing for each iteration.
  if i % 2 == 0:
    x += 30
    
  gen = gender_duration_dictionary[x]['gender']
  num = gender_duration_dictionary[x]['file_no']
  dur = gender_duration_dictionary[x]['duration']
  
  dur = float(dur)
  num = int(num)
  file_record.append([gen, num, dur])
  sound_files.append(onlyfiles[x])
  
print(file_record)
print(sound_files)

#combine files into one joined file
from pydub import AudioSegment

prev_sound = '0'

for i in sound_files:
  sound = AudioSegment.from_mp3(f"speaker_diarization/common_voice_durs/{i}")
  if prev_sound == '0':
    prev_sound = sound
  else: 
    combined_sounds = sound + prev_sound
    prev_sound = combined_sounds

combined_sounds.export("speaker_diarization/join.mp3", format="mp3")

# This cell is used to reverse the order of the chosen files' arrays
# The joined file is in reverse order compared to the arrays
# Therefore need to reverse the order of the arrays so we can use them later

file_record_rev = file_record[::-1] #reversing using list slicing
print("Resultant new reversed array:",file_record_rev)
sound_files_rev = sound_files[::-1] #reversing using list slicing
print("Resultant new reversed array:",sound_files_rev)

#convert the mp3 file into wav

from os import path
from pydub import AudioSegment

# files                                                                         
src = "/speaker_diarization/join.mp3"
dst = "/speaker_diarization/join.wav"

# convert wav to mp3                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format="wav")

filename = '/content/drive/MyDrive/finalyear/speaker_diarization/join.wav' 
#silence detecter
from pydub import AudioSegment, silence

myaudio = AudioSegment.from_wav(filename)

silence_per_arr = silence.detect_silence(myaudio, min_silence_len=1000, silence_thresh=-55)

silence_per_arr = [((start/1000),(stop/1000)) for start,stop in silence_per_arr] 

total_silence = 0
for start, stop in silence_per_arr:
  silence_time = stop - start
  total_silence = total_silence + silence_time


print(silence_per_arr)
# print('Silence is a type: ', type(silence)) # Show what the type of 'variable' is silence
print('Total silence:', total_silence)

# Get total duration of all files joined.
total_duration = 0
speech_dur_arr = []
prev_dur = 0
for i in file_record_rev:
  total_duration += i[2]
  total_duration = round(total_duration, 3)
  print('File no:', i[1], '-- Duration of file:', i[2], '// Total duration:', total_duration)
  speech_dur_arr.append((prev_dur, total_duration))
  prev_dur = total_duration

total_duration = round(total_duration, 3)
print('Total duration:', total_duration)
print(speech_dur_arr)

speech_per_arr = []
begin = 0
end = 0
dur_idx = 0

for counter, i in enumerate(silence_per_arr):
  # print(f'i: {i} / Speech_dur_arr: {speech_dur_arr[dur_idx]} / Dur_idx:  {dur_idx}')
  end = i[0]

  if i == silence_per_arr[0]:
    if i[0] < speech_dur_arr[dur_idx][1]:
      if end != 0:
        if begin != 0:
          speech_per_arr.append((begin,end))


  if speech_dur_arr[dur_idx][1] < begin:
    if dur_idx < len(speech_dur_arr)-1: dur_idx += 1

  while speech_dur_arr[dur_idx][1] < end:
    # split element
    new_end = speech_dur_arr[dur_idx][1]
    speech_per_arr.append((begin, new_end))
    begin = new_end
    dur_idx += 1

  if end != 0:
    if begin != 0:
      speech_per_arr.append((begin,end))

  if i == max(silence_per_arr):
    if total_duration > max(i):
      maxi = max(i)
      for n in range(len(speech_dur_arr)-dur_idx):
        if maxi > speech_dur_arr[dur_idx][1]:
          if dur_idx < len(speech_dur_arr)-1: dur_idx += 1
        speech_per_arr.append((maxi, speech_dur_arr[dur_idx][1]))
        maxi = speech_dur_arr[dur_idx][1]
        if dur_idx < len(speech_dur_arr)-1: dur_idx += 1
        elif dur_idx == len(speech_dur_arr)-1:
          break

  # print(speech_per_arr)
  begin = i[1]

print('Silence:', silence_per_arr)
print('Speech Dur:', speech_dur_arr)
print('Speech per:', speech_per_arr)

ground_truth = []
sil_idx = 0

for i in speech_per_arr:

  if max(silence_per_arr[sil_idx]) == min(i):
      ground_truth.append(silence_per_arr[sil_idx])
      if sil_idx < len(silence_per_arr)-1:
        sil_idx += 1

  ground_truth.append(i)


if total_duration > max(max(ground_truth)):
  ground_truth.append((max(max(ground_truth)), total_duration))
print(ground_truth)

# This cell constructs the ground truth using all previously recorded information
# The label and their duration are stored seperately in 2 different arrays.

idx = 0
sil_idx = 0
x = speech_dur_arr[idx][1] # Duration of first file
gt_labels = []
for i in ground_truth:
  if i[1] > speech_dur_arr[idx][1]:
    if idx < len(speech_dur_arr)-1:
      idx += 1

  if i[1] == silence_per_arr[sil_idx][1]:
    gt_labels.append('silence')
    if sil_idx < len(silence_per_arr)-1:
      sil_idx += 1

  elif i[0] >= speech_dur_arr[idx][0]:
    if i[1] <= speech_dur_arr[idx][1]:
      gt_labels.append(file_record_rev[idx][0])
    
       
print(len(ground_truth))
print(len(gt_labels))

for counter, i in enumerate(ground_truth):
  print(gt_labels[counter], ground_truth[counter])

mixed = []
gt_mixed = []
flag = 0
for count, (start,stop) in enumerate(ground_truth):
  print(f'Old: {start,stop}')
  # The round() function will round to the nearest integer (if not given any other arguments)
  new_start = round(start)
  new_stop = round(stop)
  difference = new_stop - stop
  print(f'New: ({new_start:.2f}, {new_stop:.2f})\n')
  if difference <= 0.6 and difference >= 0.4:
      mixed.append((new_stop-1, new_stop))
      gt_mixed.append((new_start, new_stop-1))
      gt_mixed.append((new_stop-1, new_stop))
      print(difference)
      continue
  elif difference >= -0.6 and difference <= -0.4:
      mixed.append((new_stop, new_stop+1))
      gt_mixed.append((new_start+1, new_stop))
      gt_mixed.append((new_stop, new_stop+1))
      flag = 1
      print(difference)
      continue

  # Print the new start and new stop periods, to 2 decimal places.


  # Put the newly calculated start and stop times into our silence array, so we can use it later. This is where the 'count' variable is handy
  if flag == 1:
    gt_mixed.append((new_start+1, new_stop))
  else:
    gt_mixed.append((new_start, new_stop))
  print(ground_truth,'\n')
  flag = 0

if stop > new_stop:
  gt_mixed.append((new_stop, new_stop+1))

print(gt_mixed)

print(mixed)

print(gt_labels)

gt_labels_mixed = []
labels_idx = 0
for i in gt_mixed:
  if i in mixed:
    print("mixeded")
    gt_labels_mixed.append('mixed')
  else:
    gt_labels_mixed.append(gt_labels[labels_idx])
    if labels_idx < len(gt_labels)-1:
      labels_idx += 1

print(gt_labels_mixed)

for counter, i in enumerate(gt_mixed):
  print(gt_labels_mixed[counter], gt_mixed[counter])

# Declare chunk size and get the total amount of chunks that will be generated
chunk_size = 1
total_chunks = max(gt_mixed[len(gt_mixed)-1]) / chunk_size
total_chunks = int(total_chunks)
print(total_chunks)

gt_chunks = []
chunk = 0
idx = 0
for counter, i in enumerate(range(total_chunks)):
  if chunk == max(gt_mixed[idx]):
    if idx < len(gt_mixed)-1:
      idx += 1

  if min(gt_mixed[idx]) == max(gt_mixed[idx]):
    if idx < len(gt_mixed)-1:
      idx += 1
  
  chunk += chunk_size
  gt_chunks.append(gt_labels_mixed[idx])
  print(f'{gt_labels_mixed[idx]} ({chunk-1}, {chunk}) {gt_mixed[idx]} {counter}')

print(gt_chunks)

# Loading the Libraries
from scipy.io.wavfile import read

import matplotlib.pyplot as plt

# Read the Audiofile
samplerate, data = read('/speaker_diarization/RTE_1.wav')
print(samplerate)

# Duration of the audio in Seconds
duration = len(data)/samplerate
print("Duration of Audio in Seconds", duration)
print("Duration of Audio in Minutes", duration/60)

time = np.arange(0,duration,1/samplerate)

from pydub import AudioSegment
from pydub.utils import make_chunks
import os
import glob

#need to change this number to decide how many files you want to delate
for i in range(1100):
  !rm '/content/chunk{i}.wav'


myaudio = AudioSegment.from_file("speaker_diarization/RTE_1.wav" , "wav") 
chunk_length_ms = 1000 # pydub calculates in millisec -> 1000 for 1s; 500 for 0.5s; 10 for 0.01s etc.
chunks = make_chunks(myaudio, chunk_length_ms) #Make chunks of one sec. make_chunks is a built-in function. It already exists in the library (pydub library)

#Export all of the individual chunks as wav files
counter = 0
for i, chunk in enumerate(chunks):
    chunk_name = "chunk{0}.wav".format(i)
    counter += 1
    print("exporting", chunk_name)
    chunk.export(chunk_name, format="wav")

# construct the model
model = create_model()
# load the saved/trained weights
model.load_weights("/content/gender-recognition-by-voice/results/model.h5")

# Initializing some variables to 0 as we will use them to count stuff later
male_count = 0
female_count = 0
incorrect = 0
idx = 0
silence_count = 0

for i in range(counter):
  print("\n")
  print(f'chunk{i}')
  features = extract_feature(f'/content/chunk{i}.wav', mel=True).reshape(1, -1)
  male_prob = model.predict(features)[0][0]
  female_prob = 1 - male_prob
  # predict the gender!

  myaudio = AudioSegment.from_wav(f'/content/chunk{i}.wav')
  silence_chunk = silence.detect_silence(myaudio, min_silence_len=1000, silence_thresh=-60)
  if silence_chunk:
    silence_count +=1
    gender = "silence"

  elif male_prob > female_prob:
     gender = "male"
     male_count += 1
     print(f"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}% " )

  else:
    gender = "female"
    female_count += 1
    print(f"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}% " )

  # show the result!
  print("Result:", gender)
  print("Ground truth:", gt_chunks[i])
  # --------------------------- MIXED PART ----------------------------
  if gt_chunks[i] == 'mixed':
    print('mixed')

  elif gender != gt_chunks[i]:
    incorrect += 1
    print("Incorrect!", incorrect)
  # -------------------------------------------------------------------
 
print(f"Total speaking times:      Male: {male_count} seconds.       Female: {female_count} seconds.    Silence: {silence_count} seconds ")
accuracy = round(1 - (incorrect/counter), 2)
print("Accuracy:", accuracy)



#Test with real broadcast use this code, not previous block.



# construct the model
model = create_model()
# load the saved/trained weights
model.load_weights("/content/gender-recognition-by-voice/results/model.h5")

# Initializing some variables to 0 as we will use them to count stuff later
male_count = 0
female_count = 0
incorrect = 0
idx = 0
silence_count = 0

for i in range(counter):
  print("\n")
  print(f'chunk{i}')
  features = extract_feature(f'/content/chunk{i}.wav', mel=True).reshape(1, -1)
  male_prob = model.predict(features)[0][0]
  female_prob = 1 - male_prob
  # predict the gender!

  myaudio = AudioSegment.from_wav(f'/content/chunk{i}.wav')
  silence_chunk = silence.detect_silence(myaudio, min_silence_len=1000, silence_thresh=-60)
  if silence_chunk:
    silence_count +=1
    gender = "silence"

  elif male_prob > female_prob:
     gender = "male"
     male_count += 1
     print(f"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}% " )

  else:
    gender = "female"
    female_count += 1
    print(f"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}% " )
  print("Result:", gender)
print(f"Total speaking times:      Male: {male_count} seconds.       Female: {female_count} seconds.    Silence: {silence_count} seconds ")
